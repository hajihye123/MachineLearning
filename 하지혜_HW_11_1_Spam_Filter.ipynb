{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "하지혜 - HW 11-1 Spam Filter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajihye123/Machine_Learning_Study/blob/main/%ED%95%98%EC%A7%80%ED%98%9C_HW_11_1_Spam_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zik1Z3gHtjYW"
      },
      "source": [
        "# HW 11-1. Simple Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYBPhcJuuAu"
      },
      "source": [
        "## T1. Load a dataset\n",
        "\n",
        "The following code loads a dataset consisting of text messages and spam-ham labels.\n",
        "\n",
        "You can write your own code below the **\"TODOs\"** to answer the questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UUgT0MW8Rmj"
      },
      "source": [
        "### Questions:\n",
        "* Number of spam messges? [*747*]\n",
        "* Number of ham messages? [*4825*]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2napWf8tINR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2be867-b89b-44a0-8dc9-68a6f7982163"
      },
      "source": [
        "from typing import List, Tuple, Dict, Iterable, Set\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mlee-pnu/IDS/main/spam_dataset.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# TODOs\n",
        "print(df.head())\n",
        "\n",
        "message = df.groupby(['Category']).count() \n",
        "print(message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Category                                            Message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "          Message\n",
            "Category         \n",
            "ham          4825\n",
            "spam          747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5wryED8lOBP"
      },
      "source": [
        "## T2. Spam filter for individual words\n",
        "\n",
        "We first defined a function ***tokenize()*** to convert a given text into a set of words. \n",
        "\n",
        "Using the function, we now try to count the frequency of each word in each class (spam and ham).\n",
        "\n",
        "Complete the following code and answer the following questions:\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9VmU28wPHcH"
      },
      "source": [
        "### Qeustions: \n",
        "*   Number of spam messages containing the word \"free\": [*170*]\n",
        "\n",
        "Let's assume we only care for the word \"free\" to determine if a message is a spam or not. Answer the following questions:\n",
        "\n",
        "*   P ( *ham* | *free* ) = [*0.2576419213973799*]\n",
        "*   Is this message spam? [*Yes*]\n",
        "\n",
        "*Note: Do not apply a smoothing method here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6rmDIBJnFNh"
      },
      "source": [
        "def tokenize(text: str) -> Set[str]:\n",
        "    text = text.lower()                         \n",
        "    all_words = re.findall(\"[a-z0-9']+\", text)  \n",
        "    return set(all_words)                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtcK6qXlrES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3e1231-4230-4889-8e98-12ae59905dd1"
      },
      "source": [
        "tokens: Set[str] = set()\n",
        "token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "spam = df[df.Category == 'spam']\n",
        "ham = df[df.Category == 'ham']\n",
        "\n",
        "for msg in spam['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_spam_counts[token] += 1\n",
        "\n",
        "for msg in ham['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_ham_counts[token] += 1\n",
        "\n",
        "# TODOs\n",
        "word = \"free\"\n",
        "n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "n_word_ham = token_ham_counts[word] # frequency of the word in ham messages\n",
        "\n",
        "p_spam = spam['Message'].count() / (spam['Message'].count() + ham['Message'].count()) # P(spam)\n",
        "p_ham =  ham['Message'].count() / (spam['Message'].count() + ham['Message'].count()) # P(ham)\n",
        "\n",
        "p_word_given_spam = n_word_spam / spam['Message'].count() # P(word|spam)\n",
        "p_word_given_ham = n_word_ham / ham['Message'].count() # P(word|ham)\n",
        "\n",
        "# P(spam|word)\n",
        "p_spam_given_word = p_word_given_spam * p_spam / ( (n_word_spam + n_word_ham) / (spam['Message'].count() + ham['Message'].count()))\n",
        "print(p_spam_given_word)\n",
        "# P(ham|word)\n",
        "p_ham_given_word = p_word_given_ham * p_ham / ( (n_word_spam + n_word_ham) / (spam['Message'].count() + ham['Message'].count()))\n",
        "print(p_ham_given_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.74235807860262\n",
            "0.2576419213973799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUzCC6l6knfU"
      },
      "source": [
        "## T3. Spam filter that combines words: Naive Bayes\n",
        "\n",
        "You received a text message \"just do it\" from an unknown sender.\n",
        "\n",
        "Complete the function ***predict()*** that outputs the probability of the message being spam and the predicted label of the message. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_a7X9Mj4CIs"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "*   P ( *spam* | *text* ) = [*0.002791028835884658*]\n",
        "*   Is this text message spam? [*No*]\n",
        "\n",
        "*Note: You do not need to apply a smoothing method here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDTL_uBGL86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103d0271-660c-4997-f167-57fcad08bf53"
      },
      "source": [
        "text = \"just do it\"\n",
        "\n",
        "# TODOs\n",
        "def predict(text: str):\n",
        "  message = text.split()\n",
        "  for word in message:\n",
        "    n_word_spam = token_spam_counts[word]\n",
        "    n_word_ham = token_ham_counts[word]\n",
        "    p_spam = spam['Message'].count() / (spam['Message'].count() + ham['Message'].count()) # P(spam)\n",
        "    p_ham =  ham['Message'].count() / (spam['Message'].count() + ham['Message'].count()) # P(ham)\n",
        "    p_word_given_spam = n_word_spam / spam['Message'].count() # P(word|spam)\n",
        "    p_word_given_ham = n_word_ham / ham['Message'].count() # P(work|ham)\n",
        "    p_spam_given_word = p_word_given_spam * p_spam / ( (n_word_spam + n_word_ham) / (spam['Message'].count() + ham['Message'].count())) # P(spam|word)\n",
        "    p_ham_given_word = p_word_given_ham * p_ham / ( (n_word_spam + n_word_ham) / (spam['Message'].count() + ham['Message'].count())) # P(ham|word)\n",
        "    p_spam_given_word *= p_spam_given_word\n",
        "    p_ham_given_word *= p_ham_given_word\n",
        "  if p_ham_given_word > p_spam_given_word:\n",
        "    print('P(spam|text):',p_spam_given_word) # P(spam|text)\n",
        "    label = \"ham\"\n",
        "    return p_ham_given_word, label\n",
        "  elif p_ham_given_word < p_spam_given_word:\n",
        "    label = \"spam\"\n",
        "    return p_spam_given_word, label\n",
        "  else:\n",
        "    label = \"equal\"\n",
        "    return p_ham_given_word, label\n",
        "\n",
        "print(predict(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P(spam|text): 0.002791028835884658\n",
            "(0.8971306514773941, 'ham')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQ3M91eTG7I"
      },
      "source": [
        "## T4. Smoothing method\n",
        "\n",
        "You again received two text messages from unknown senders.\n",
        "\n",
        "Complete the function ***spamFilter()*** that classifies a given message. \n",
        "\n",
        "You may want to apply a smoothing method for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IBahJB5Tlat"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "*   Is textA spam?: [*Yes*]\n",
        "*   Is textB spam?: [*No*]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ66DoXVTpR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08321ec2-7047-4085-b363-eea372dcf36e"
      },
      "source": [
        "textA = \"reward! download your free ticket from our website www.pnu.edu\"\n",
        "textB = \"call me and get your money back\"\n",
        "\n",
        "# TODOs\n",
        "n_spam = sum(token_spam_counts.values())\n",
        "n_ham = sum(token_ham_counts.values())\n",
        "\n",
        "# n_tokens\n",
        "n_tokens = len(tokens)\n",
        "\n",
        "# Initiate parameters\n",
        "parameters_spam = {unique_word:0 for unique_word in tokens}\n",
        "parameters_ham = {unique_word:0 for unique_word in tokens}\n",
        "\n",
        "alpha = 1\n",
        "\n",
        "# Calculate parameters\n",
        "for word in tokens:\n",
        "  n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "  p_word_given_spam = (n_word_spam + alpha) / (n_spam + alpha * n_tokens)\n",
        "  parameters_spam[word] = p_word_given_spam\n",
        "\n",
        "  n_word_ham = token_ham_counts[word] # frequency of the word in spam messages\n",
        "  p_word_given_ham = (n_word_ham + alpha) / (n_ham + alpha * n_tokens)\n",
        "  parameters_ham[word] = p_word_given_ham\n",
        "\n",
        "def spamFilter(text: str):\n",
        "  message = text.split()\n",
        "  \n",
        "  p_spam_given_message = p_spam\n",
        "  p_ham_given_message = p_ham\n",
        "\n",
        "  for word in message:\n",
        "    if word in parameters_spam:\n",
        "      p_spam_given_message *= parameters_spam[word]\n",
        "\n",
        "    if word in parameters_ham: \n",
        "      p_ham_given_message *= parameters_ham[word]\n",
        "\n",
        "  print('P(spam|message):', p_spam_given_message)\n",
        "  print('P(ham|message):', p_ham_given_message)\n",
        "\n",
        "  if p_ham_given_message > p_spam_given_message:\n",
        "    label = \"ham\"\n",
        "    return label\n",
        "  elif p_ham_given_message < p_spam_given_message:\n",
        "    label = \"spam\"\n",
        "    return label\n",
        "  else:\n",
        "    label = \"equal\"\n",
        "    return label\n",
        "  \n",
        "print(spamFilter(textA))\n",
        "print(spamFilter(textB))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P(spam|message): 9.950760035451472e-23\n",
            "P(ham|message): 4.566291033531711e-24\n",
            "spam\n",
            "P(spam|message): 2.7050714264181724e-20\n",
            "P(ham|message): 6.1048710578548686e-18\n",
            "ham\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}